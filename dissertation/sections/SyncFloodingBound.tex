\section{Bounding the synchronous flooding time}
\label{SyncFloodingSection}

In this section we consider a new algorithm for rumour spreading on dynamic network known as flooding, which takes place in discrete rounds instead of continuous time. First we introduce algorithm and prove an upper bound for the spreading time on a dynamic network from \cite{syncPaper}. Then we introduce a new type of dynamic network, where the sequence of topologies is not deterministic, but instead follows a distribution. We generalise our flooding bound to this probabilistic class of dynamic networks, and see an extended example of applying the bound.

\subsection{Model}

First we introduce a new algorithm for rumour spreading in discrete rounds (referred to in \cite{asyncPaper} as synchronous spread).

\begin{definition} \label{SyncFloodingAlgorithm}
	Synchronous Flooding Algorithm

	\noindent
	The synchronous flooding algorithm proceeds in rounds on a dynamic network $\mathcal{G} = (G_t)_{t \in \mathbb{N}}$. Initially a single node is aware of the rumour. In round $i$, every node that is aware of the rumour informs all of its neighbours in $G_i$, regardless of whether they already knew the rumour or not.
\end{definition}

We note that the asynchronous rumour spreading algorithm (Algorithm \ref{NodeCentricAsyncAlgorithm}) was probabilistic since the algorithm depended on sources of randomness such as exponential clocks. In contrast, once the dynamic network and initial node from which the rumour spreads are chosen, Algorithm \ref{SyncFloodingAlgorithm} is entirely deterministic. Since the trajectory of the rumour spread is completely determined by the initial conditions, it is reasonable to ask why we would want bounds on the spreading time if we can simply simulate the rumour, and record how long it takes to inform all the nodes. In some cases however, the graph may be so large that simulation is computationally infeasible. 
In such cases, we may be able to take advantage of bounds in terms of the structural properties of graphs instead. % TODO: Said "cases" at beginning of both sentances
Additionally, in section \ref{subsection:MEDNBound}
we analyse rumour spreading on a non-deterministic dynamic network, where the sequence of graphs is not fixed but instead follows some distribution. 
In this case, deterministic bounds will allow us to bound spreading time over the distribution of graph sequences. %TODO: Said "case" again - change

% TODO: Why study flooding time?
% TODO: mention that Flooding only works for synchronus


\subsection{Bound on spreading time for the Flooding Algorithm}

In this section we prove a bound on the spreading time of the flooding algorithm from \cite{syncPaper}, adding details and expanding steps throughout.

First we introduce the notion of an expander graph, which we need to express the bound.

\begin{definition}
	Boundary set $B(S)$

	\noindent 
	Given a graph $G=(V,E)$, the boundary set $B(S)$ of a vertex subset $S \subseteq V$ is defined as the set of vertices outside $S$ connected to a node of $S$ by an edge, i.e
	$$
		B(S) = \left\{u \in V \setminus S : \{u, v\} \in E \text{ for some } v \in S \right\}
	$$
\end{definition}

% TODO: Figure of boundary sets

\begin{definition}
	$(h, k)$-Expander

	\noindent
	A graph $G$ on a vertex set $V$ is a $(h, k)$-expander if all subsets of the nodes $S \subseteq V$ such that $|S| \leq h$ satisfy $|B(S)| \geq k|S|$.
\end{definition}

Intuitively, an $(h, k)$-Expander should be well-connected since each "small" subset of nodes (i.e. less than $k$ nodes) is connected to at least $k$ other nodes outside the set.  

We are now ready give a bound on the rumour spreading time for the flooding algorithm.

\begin{theorem}\label{theorem:DeterministicFloodingBound}
	\ModelIntro Suppose also that there exists an increasing sequence $1 = h_0 \leq h_1 < \dots < h_s = \frac{n}{2}$ and decreasing sequence $k_1 \geq k_2 \geq \dots \geq k_s$ of positive real numbers, such that for all $t \in \mathbb{N}$, $G_t$ is a $(h_i, k_i)$-expander for every $i \in \{1, \dots , s\}$. Then the spreading time of $\mathcal{G}$ under algorithm \ref{SyncFloodingAlgorithm} is
	$$
		\mathcal{O}\left(\sum_{i=1}^s \frac{\log \frac{h_i}{h_{i-1}}}{\log(1+k_i)}\right)
	$$
\end{theorem}

% TODO: Interpretation of how the bound varies for different (h_i, k_i) sequences, how do these emerge?

We notice that there are strong conditions on the connectivity of the dynamic network that must be met for this bound to hold, namely that every topology is an $(h,k)$-expander for multiple $(h, k)$ pairs. Initially these conditions may seem unreasonably strong, however in Section \ref{subsect:floodingBoundEdgeMarkovian} we will see an interesting example where these properties emerge w.h.p in a random graph.

\begin{proof}
	For $i = 1,\dots, s$, let $T_i$ be the earliest time step such that the number of informed nodes is larger than $h_i$, i.e

	$$
		T_i = \min \{ t \in \mathbb{N} : |I_t| > h_i \}
	$$

	First we upper-bound the number of steps in the interval $[T_i, T_j]$, % TODO: Generalise to [T_i, T_j], should all hold as expected
	by bounding the number of informed nodes $t$ time-steps after $T_i$ from below.

	By induction, we show that if $T_i + t < T_j$, then the number of informed nodes at time $T_i + t$ is greater than $(1+k_j)^t h_i$.
	
	\underline{Base Case.} Suppose that $t=0$. 
	
	Then by the definition of $T_i$, the number of informed nodes at time $T_i$ is strictly greater than $h_i$

	\underline{Inductive Case.} For brevity, let $s = T_i + t$. Suppose that $|I_s| > (1+k_j)^t h_i$.

	We notice that 
	\begin{align*}
		|I_{s+1}| &= |I_s| + |B(I_s)| \\
		\intertext{by the operation of Algorithm \ref{SyncFloodingAlgorithm}}
		& \geq |I_s| + k_j |I_s| \\ % since we assume s < T_j => |I_s| \leq h_j + (h_j, k_j)-expander
		\intertext{as $G_t$ is a $(h_j, k_j)$-expander, so we observe that $|B(I_s)| \leq h_j$ since $s < T_j$}
		& = (1 + k_j)|I_s| \\
		& > (1 + k_j)(1+k_j)^t h_i \\
		\intertext{by the induction hypothesis}
		& = (1+k_j)^{t+1} h_i
	\end{align*}
	% TODO: boundary set picture here	
	Let $\tau = T_i + \ceil{ \frac{\log (h_j/h_i)}{\log(1+k_j)}}$. 
	Suppose for contradiction that $\tau < T_j$. %TODO: Is contradicition overkill here?
	Since we assumed $\tau < T_j$ we can apply the above inequality %TODO: label
	to get the following lower bound on the number of informed nodes at time $\tau$.
	$$
		|I_\tau| > 
		(1+k_j)^{\ceil{ \frac{\log (h_j/h_i)}{\log(1+k_j)}}} h_i
		\geq e^{\log (h_j/h_i)} h_i
		= h_j
	$$
	Thus we have a contradiction, since under the assumption that $\tau < T_j$, we get that the number of informed nodes at time $\tau$ is greater than $h_j$, i.e. by the definition of $T_j$, $\tau \geq T_j$. Hence, we have that $\tau \geq T_j$ i.e. $\ceil{ \frac{\log (h_j/h_i)}{\log(1+k_j)}}$ time-steps after $T_i$, the number of informed nodes is greater than $h_j$. Rearranging this inequality gives us the following bound on the length of the interval $[T_i, T_j]$:
	\begin{equation} \label{eq:FloodingIntervalBound}
		T_j - T_i \leq \ceil{ \frac{\log (h_j/h_i)}{\log(1+k_j)}} \leq \frac{\log (h_j/h_i)}{\log(1+k_j)} + 1
	\end{equation}
	% TODO: Make ceiling brackets fit
	% TODO: Exposition about next steps + direction here

	% TODO: What about time until first spread - must happen at timestep 2 since (h,k) expander so no isolated nodes.
	Note that since $h_s = \frac{n}{2}$, at time $T_s$ at least half of the nodes will be aware of the rumour. Hence, if we can bound $[T_0, T_s]$, we will have bounded the time it takes for half of the nodes to be informed. 

	To bound $[T_0, T_s]$, we bound each subinterval $[T_{i-1}, T_i]$. Naively applying the bound to each subinterval directly and taking the sum yields the overall bound 
	$$
		\mathcal{O}\left(\sum_{i=1}^s \frac{\log \frac{h_i}{h_{i-1}}}{\log(1+k_i)} + s\right)
	$$ 
	for the interval $[T_0,T_s]$. However, we can tighten the bound by removing the additive $s$ term through case analysis on the subinterval $[T_{i-1}, T_i]$, as follows.

	% TODO: Link back to theorem, after since n/2 = h_s, after T_s at least n/2 nodes are informed


	% TODO: Note that the source of the problems is the cieling function - aim to get into form like in thm statement, but can't naively add all togethe oterwise + O(S) error, requires deeper analysis

	\textbf{Case 1.} Suppose $\frac{\log (h_i/h_{i-1})}{\log(1+k_i)} \geq 1$.

	In this case $\frac{\log (h_i/h_{i-1})}{\log(1+k_i)}$ term is the dominant term in inequality (\ref{eq:FloodingIntervalBound}), thus
	$$
		T_i - T_{i-1} = \mathcal{O}\left( \frac{\log (h_i/h_{i-1})}{\log(1+k_i) }\right) + \mathcal{O}(1) = \mathcal{O}\left( \frac{\log (h_i/h_{i-1})}{\log(1+k_i) }\right)
	$$

	\textbf{Case 2.} Suppose $\frac{\log (h_i/h_{i-1})}{\log(1+k_i)} < 1$.

	In this case we cannot achieve the same bound on $T_i - T_{i-1}$, since the additive 1 is the dominant term. By following the same reasoning as in Case 1, instead we can only obtain that
	\begin{equation}\label{eq:Weak1StepBound}
		T_i - T_{i-1} = \mathcal{O}\left( \frac{\log (h_i/h_{i-1})}{\log(1+k_i) }\right) + \mathcal{O}(1) = \mathcal{O}(1)
	\end{equation}
	To understand the situation that this case represents, we inspect the inequality we assumed.
	By rearranging we find that $(1+k_i)h_{i-1} > h_i$. Since we have assumed that $T_{i-1} < T_i$, % TODO: Assume this earlier
	the number of informed nodes at the start of round $T_{i-1}$ satisfies
	$$
		|I_{T_{i-1}}| \leq h_i
	$$
	Hence, as the topology is an in $(h_i, k_i)$-expander, in round $T_{i-1}$ at least $|I_{T_{i-1}}|k_i$ new uninformed nodes on the boundary of the informed node set are informed. Thus, at the start of the next round, there are at least $(1+k_i)h_{i-1}$ informed nodes, since $|I_{T_{i-1}}| > h_{i-1}$ by the definition of $T_{i-1}$. Combining this with the rearranged form of the assumed inequality, we see that the number of informed nodes "jumps" to more than $h_i$ nodes in a single time step.

	It may be that after this jump there are also more than $h_j$ nodes for some $j \geq i$, %TODO: insert figure here of t vs |I_t|, for t = T_{i-1}, T_i, T_{i+1}, etc, maybe similar figure for steps in case 1.
	from which we obtain $T_{i-1} + 1 = T_i = T_{i+1} = \dots = T_j$. See Figure \ref{fig:floodingJump} for an illustration of this "jump" behaviour. 
	\begin{figure}[h]
		\centering
		\includegraphics[width=1\textwidth]{./figures/flooding_jump.png}
		\caption{Behaviour of the number of informed nodes in the "jump" case}
		\label{fig:floodingJump}
	\end{figure}
	In this case the total time that has passed between $T_{i-1}$ and $T_j$ is 1, so we instead bound the whole jump interval as follows:
	
	% TODO: j doesn't exist case
	Let $j$ be the index such that $h_j < (1+k_i)h_{i-1} \leq h_{j+1}$, i.e. the last $j \geq i$ when $T_j = T_{i-1} + 1$, as in Figure \ref{fig:floodingJump}. % TODO: Check this equivalence - think it does hold
	% TODO: Figure here to illusttrate T_j, 
	% TODO: "may be the same as before" - what does this mean??? 
	Hence, by the definition of $j$, h
	\begin{align*}
		T_j - T_{i-1} &=1 \\ 
		&\leq \frac{\log (h_{j+1}) - \log(h_{i-1})}{\log(1+k_i) } \\
		& =\sum_{l=i}^{j+1} \frac{\log (h_{l}) - \log(h_{l-1})}{\log(1+k_i) } & \text{since the numerator forms a telescoping sum} \\
		& \leq \sum_{l=i}^{j+1} \frac{\log (h_{l}/h_{l-1})}{\log(1+k_l) } & \text{since } k_i \geq k_l \text{ for all } l \geq i \\
		& = \mathcal{O}\left(\sum_{l=i}^j \frac{\log (h_{l}/h_{l-1})}{\log(1+k_l) }\right)
 	\end{align*}

	We now use this equality to bound the growth of $T_s$, the first time at which more than $h_s = \frac{n}{2}$ nodes are aware of the rumour. We can express $T_s$ as a telescoping sum of intervals as follows:
	$$
		T_s = \sum_{i=1}^s T_i - T_{i-1}
	$$
	For the $i^\text{th}$ interval, if $\frac{\log (h_i/h_{i-1})}{\log(1+k_i)} \geq 1$, we can apply the result from Case 1. If $\frac{\log (h_i/h_{i-1})}{\log(1+k_i)} < 1$, we apply Case 2 to bound the whole jump interval $[T_{i-1}, T_j]$, and continue the summation from $T_j$. % TODO: CHeck, problems from j+1 on case 2 result?

	TODO: FINISH PROOF
	
\end{proof}

\subsection{Review of Markov Chains}

\begin{enumerate}
	\item Definition
	\item Stationary distribution definition and Interpretation
\end{enumerate}

\subsection{Flooding on Markovian Evolving Dynamic Networks}

Until now, all the dynamic networks we have studied have been deterministic, in the sense that the evolution of topologies has been fixed. However, knowing the future topologies of the network may be unrealistic in practice. For example, when considering rumour spread on a real-world network such as the internet, we cannot say with certainty what connections will be present on a given time in the future. This motivates a generalistion of the dynamic network model where the sequence of topologies is drawn from some distribution. This distribution expresses our uncertainty in the future topologies of the network, but allows us to encode information about how we predict the shape of the network may evolve. Under this model, the dynamic network itself is a stochastic process, and a source of randomness. In this section, we analyse the rumour spreading time when a Markov chain is used to  generate the random sequence of topologies.

First we introduce a variation of the original Dynamic Network definition.

\label{subsection:MEDNBound}

\begin{definition}
	Markovian Evolving Dynamic Network (MEDN)

	\noindent 
	Let $\mathcal{A}(V)$ be the set of all possible graphs on the vertex set $V$.
	A Markovian Evolving Dynamic Network is a random sequence of graphs $\mathcal{M} = (G_t)_{t \in \mathbb{N}}$ indexed by an integer time $t$, such that $\mathcal{G}$ is a Markov chain with state space $\mathcal{A}(V)$.
\end{definition}

We see that this definition adheres to the previous restriction that all topologies have the same vertex set. Since the network is Markovian, the distribution from which the next topology is draw depends only on the previous topology.

We say that a given MEDN $\mathcal{M}$ is stationary if the distribution of $G_0$ is a stationary distribution of $\mathcal{M}$. By the definition of a stationary distribution, in such an MEDN the probability that $G_t = G'$ for a given $G'$ is the probability that $G_t = G'$ under the stationary distribution, for any time step $t$. Hence, once the Markov chain associated with the network has reached equilibrium, % TODO: define "equalibrium"
the future evolution of the network effectively proceeds by independently sampling a topology from the stationary distribution at each time step. % TODO: Check if independent, get rid of "effectively"?

Under certain conditions, Markov chains converge to their stationary distributions. Thus, for the rest of this section we assume that our MEDNs have been running for a long enough such that when we inject the rumour the network is already in equilibrium. In this paper we don't discuss the conditions which allow for converge to the stationary distribution or how long the convergence takes, and assume that round 0 begins when we inject the rumour.
% TODO: What about if we started not in the stationary distributuion?? Further exploration needed. Or close to stationary? 

We now adapt the deterministic bound proved in Theorem \ref{theorem:DeterministicFloodingBound} to a MEDN, by adding details to proof from \cite{syncPaper}

% TODO: Is this a non-theorm? What does it acutally mean?

\begin{theorem}\label{theorem:markovSyncBound}
	Let $\mathcal{M} = (G_t)_{t \in \mathbb{N}}$  be a stationary Markovian evolving graph. Suppose also that there exists an increasing sequence $1 = h_0 \leq h_1 < \dots < h_s = \frac{n}{2}$ and decreasing sequence $k_1 \geq k_2 \geq \dots \geq k_s$ of positive real numbers, such that with probability $1-\frac{1}{n^2}$, the stationary distribution of $\mathcal{M}$ is a $(h_i, k_i)$-expander for every $i \in \{1, \dots , s\}$. Then the spreading time of $\mathcal{G}$ under algorithm \ref{SyncFloodingAlgorithm} is
	$$
		\mathcal{O}\left(\sum_{i=1}^s \frac{\log \frac{h_i}{h_{i-1}}}{\log(1+k_i)}\right)
	$$
	w.h.p.
\end{theorem}

Note that in previous bounds the source of randomness in the spreading time has been the rumour spreading algorithm, however in this bound the only source of randomness is the network itself.

We notice that in this theorem, instead of requiring every graph topology to satisfy the expansion conditions of Theorem \ref{theorem:DeterministicFloodingBound}, we only require that the same expansion properties hold w.h.p with respect to the stationary distribution. % TODO: REWORD??
Since each topology is sampled from the stationary distribution, these properties will hold for nearly all the topologies in the sequence. Thus, this theorem states that our deterministic bound will hold for MEDNs, as long as the long-term proportion of topologies in our network without strong expansion properties is bounded by $\frac{1}{n^2}$.

% TODO: Review this proof for order that makes sense, and check it actually holds

\begin{proof}
	For all $t \in \mathbb{N}$, let $A_t$ be the event that $G_t$ is an $(h_i, k_i)$-expander for all $1 \leq i \leq s$. Since $\mathcal{M}$ is stationary, we have that $\mathbb{P}(A_t) \geq 1 - \frac{1}{n^2}$ for all t. 

	Let $B$ denote the event we are interested in, namely that the rumour spreading time is 	
	$$
		\mathcal{O}\left(\sum_{i=1}^s \frac{\log \frac{h_i}{h_{i-1}}}{\log(1+k_i)}\right)
	$$

	Suppose that $A_k$ holds for all $k \leq n$. We claim that the rumour spreads in at most $n$ time steps with certainty. Since we have assumed that all topologies up to time $n$ are $(h_1, k_1)$-expanders, there are no isolated nodes, as such an isolated node $v$ would form a subset of size $1 \leq h_1$ with $B({v}) = 0 < k_1$, a contradiction. Thus, the network is connected for the first $n$ time steps. Hence, for any time step $t \leq n$ before all nodes are informed of the rumour, $B(I_t) \geq 1$, since there is only 1 connected component so $I_t$ must be connected  to $U_t$ by at least 1 edge. It follows that in each time step at least 1 new node becomes informed of the rumour by the operation of algorithm \ref{SyncFloodingAlgorithm}. Hence, at most $n$ steps are needed to spread the rumour when $A_k$ holds for all $k \leq n$. 

	Let $T$ denote the first time step at which all nodes are informed of the rumour. By Theorem \ref{theorem:DeterministicFloodingBound} we have that if $A_k$ holds for all $k \leq T$, then $T=\mathcal{O}\left(\sum_{i=1}^s \frac{\log (h_i/h_{i-1})}{\log(1+k_i)}\right)$, as once all the nodes have been informed, the expansion properties of the remaining topologies are irrelevant.

	Now we combine the previous two claims. By the first claim if $A_k$ holds for all $k \leq n$ we have that $T \leq n$, thus $A_k$ holds for all $k \leq T$. In this case, by the second claim we obtain that  $T=\mathcal{O}\left(\sum_{i=1}^s \frac{\log (h_i/h_{i-1})}{\log(1+k_i)}\right)$.

	Since we have shown that the event $\bigcap_{t=0}^n A_t \implies B$, we have that $\comp{B} \implies \comp{ \bigcap_{t=0}^n A_t} = \bigcup_{t=0}^n \comp{A_t}$. To finish the proof, we show that the event $\comp{B}$ occurs with vanishing probability, thus $B$ occurs w.h.p.

	\begin{align*}
		\mathbb{P}(\comp{B}) &\leq \mathbb{P}(\bigcup_{t=0}^n \comp{A_t}) & \text{by the inferred implication} \\ 
		& \leq n \mathbb{P}(\comp{A_t}) & \text{by the union bound}\\ 
		& \leq n \frac{1}{n^2} = \frac{1}{n} & \text{by the conditions on the stationary distribution}
	\end{align*}

	% TODO: THINK ABOUT h_0

\end{proof}

\subsection{Applying the bound to the Edge Markovian MEDN}\label{subsect:floodingBoundEdgeMarkovian}

In this section, we apply the bound proved in Theorem \ref{theorem:markovSyncBound} to a specific MEDN called the Edge-Markovian Network. We will also evaluate the quality of the bound by proving a lower bound on the spreading time, and comparing the results with simulations. 

First, we introduce and motivate the model.
\begin{definition}
	Edge-Markovian Network $\mathcal{M}(n, p, q)$

	\noindent
	Let $V$ be a vertex set, and $E = \left\{e : e \subseteq V, |e| = 2 \right\}$ be the set of all possible edges between vertices in $V$. 
	Let $\left\{\{X_e(t)\}_{t \in \mathbb{N}} : e \in E \right\}$ be a set of independent Markov chains on the state space $\{0,1\}$, each with the transition matrix
	\begin{center}
		\begin{tabular}{ c | c c }
		   & 0     & 1 \\ 
		\hline
		 0 & $1 - p$ & $p$ \\  
		 1 & $q$     & $1 - q$  
		\end{tabular}
	\end{center}
	An Edge-Markovian Evolving Network $\mathcal{M}(n, p, q) = \left\{(V, E_t) : t \in \mathbb{N} \right\}$ is a random sequence of graphs on the vertex set $V$ where $E_t = \left\{ e \in E : X_e(t) = 1 \right\}$, i.e. each edge $e$ is only present in the time steps when its associated Markov chain $X_e$ is in state 1.
\end{definition}
The Edge-Markovian Network models the situation when connections between nodes independently fail or are restored. The "birth" and "death" of each edge $e$ is dictated by its associated Markov chain $X_e$, where in each round, a present edge has probability $q$ of failing, and a missing edge has probability $p$ of restoration. Hence, for any given edge, after a geometrically distributed time with parameter $q$, a failure occurs. Restoring the edge takes a geometrically distributed time with parameter $p$ % TODO: WHAT DO FAILING AND RESTORATION MEAN

Since the future state of each edge is only dependent on current state, this Network is appropriate for modelling memoryless Networks where we can assume each edge acts independently with identical dynamics to other edges. For example, in a computer network we could imagine each  connection is randomly disrupted by an issue with probability $q$, and the affected connection is fixed with probability $p$ in each of the subsequent time steps. 

Now we will see that the set of Markov chains associated with the edges induces a Markov chain over the set of possible graphs on $V$, thus the Edge-Markovian Network is an MEDN.

% TODO: REPAIR-DELAY NETWORKS - any edge could fail, takes a few steps to fix, how to model, what is the stationary distribution?

\begin{lemma}
	The Edge-Markovian Network is a MEDN
\end{lemma}

\begin{proof}
	Since all the edges are memoryless, the distribution of which edges will be present in the subsequent round is only dependent on which edges are present currently. %TODO: Reword this
	Thus, the distribution over possible topologies in the following round is only dependent on the topology in the current round.
	Hence, the sequence of topologies is a Markov chain, so the network is an MEDN.
	% TODO: Include transition probabilities
\end{proof}

% TODO: Proof that it is a MEG 

% TODO: Proof of stationary distribution - plug into

Since the Edge-Markovian Network is an MEDN, we can apply Theorem \ref{theorem:markovSyncBound} to bound the spreading time of Algorithm \ref{SyncFloodingAlgorithm}. Before applying the bound, we need to establish the stationary distribution of this network. First we establish when a stationary distribution exists for the Edge-Markovian MEDN.

\begin{lemma}
	An Edge-Markovian MEDN $\mathcal{M}(n, p, q)$ has a unique stationary distribution if $0 < p, q < 1$.
\end{lemma}

\begin{proof}
	If either $p=0$ or $q=0$, then the edges could get stuck in a present or not-present state, but if $0 < p, q$ then the Markov chain can always move to a new state. When we also have that $p, q < 1$, any set of insertions and deletions from an arbitrary state has non-zero probability. Thus, under these conditions, it is possible to transition between any two states in a single time step. Hence, the Markov chain is irreducible when $0 < p, q < 1$. 

	If $p=q=1$, then the Markov chain is periodic as edges alternate between present and non-present each time step. However, if $p, q < 1$ no such periodic behaviour can take place with certainty, so the Markov chain is aperiodic.

	Hence, if $0 < p, q < 1$, then the Markov chain is irreducible and aperiodic, thus by % TODO: CITE markov chain theorem
	it has a unique stationary distribution.
\end{proof}

For convenience, we assume that $0 < p, q < 1$ for the remainder of this section. Now we introduce the unique stationary distribution for the Edge-Markovian MEDN. 

\begin{definition}
	Erdős-Rényi Random Graph $G(n,\tilde{p})$

	\noindent	
	The Erdős-Rényi (ER) Random Graph is a distribution over all the possible graphs on a vertex set of size $n$, where each edge is present with probability $\tilde{p}$.
\end{definition}

Now we prove that the ER distribution $G(n,\tilde{p})$ is the stationary distribution for the Edge-Markovian MEDN for a specific $\tilde{p}$.

\begin{lemma}
	The stationary distribution of $\mathcal{M}(n, p, q)$ is the Erdős-Rényi distribution $G(n, \frac{p}{p+q})$ 
\end{lemma}

\begin{proof}
	Let 
	$$
		\pi = \left(\frac{q}{p+q}, \frac{p}{p+q}\right)
	$$ 
	be a distribution over the starting states of $X_e$ for any $t \in \mathbb{N}, e \in E$, % TODO: E not defined here
	where the first component is the probability of starting in state 0, and the second component is the probability of starting in state 1. First we show that $\pi$ is the stationary distribution for all the edge Markov chains.

	Let $M$ be the transition matrix for any of the edge Markov chains. We calculate the distribution over $\{0,1\}$ after a single round starting in distribution $\pi$. Observe that
	$$
		\pi M =
		\begin{bmatrix}
			\frac{q}{p+q} & \frac{p}{p+q}
		\end{bmatrix}
		\begin{bmatrix}
			1 - p & p \\
			q & 1 - q 
		\end{bmatrix}
		= \begin{bmatrix}
			\frac{q}{p+q} & \frac{p}{p+q}
		\end{bmatrix}
		= \pi
	$$
	thus $\pi$ is the stationary distribution of any $X_e$.
	
	Now we formulate the stationary distribution of the topology Markov chain. Note that this is a distribution over possible topologies on the vertex set $V$. When all edges start in the stationary distribution, at any time step $t$, $X_e(t) = 1$ with probability $\frac{p}{p+q}$ by the stationarity of $\pi$. Hence, under this initial distribution, each edge is present with probability $\frac{p}{p+q}$ in any time step. Note that this distribution over the possible topologies is not dependent on the topology of the network at any other time, hence it is stationary. % TODO: IS this clear direction -> distribution is stationary

	We recognise this distribution as the $G(n, \frac{p}{p+q})$ ER Random Graph distribution.
\end{proof}

Thus, when we start the Markov chain in a topology chosen at random according to the $G(n, \frac{p}{p+q})$ distribution, the distribution of topologies for the next step is again the $G(n, \frac{p}{p+q})$ distribution. Inductively, the distribution of the topology of the network at any time step is $G(n, \frac{p}{p+q})$.

To apply Theorem \ref{theorem:markovSyncBound}, we need to compute expansion properties of the stationary distribution that hold w.h.p. We do this for an arbitrary $G(n, p)$ distribution.

\begin{lemma}\label{lemma:GnpExpansion}
	Let $p \geq c\frac{\log n}{n}$ for a sufficiently large constant $c$. For a $G(n, p)$ graph we have that for all $S \subseteq V$ such that $|S| \leq \frac{n}{2}$
	$$
		|B(S)| \geq \frac{n}{c} \min \left\{ |S|p, 1 \right\} 
	$$
	with probability at least $1 - \frac{1}{n^2}$, for large $n$. 
\end{lemma}

% TODO: mention OMit proof citing elementary probability + not relevant

Lemma \ref{lemma:GnpExpansion} tells us that if we choose a random topology according to the $G(n, p)$ distribution with a suitably large $p$, w.h.p the boundaries of all "small" sets are large. Note that the condition on $p$ scales with the number of nodes $n$. % TODO: Expanding

In Lemma \ref{lemma:GnpExpander}, we use Lemma \ref{lemma:GnpExpansion}, to establish expansion properties for the $G(n,p)$ distribution.

% TODO: Interpretation of lemma in terms of expander graphs

\begin{lemma}\label{lemma:GnpExpander}
	Let $p \geq c\frac{\log n}{n}$ for a sufficiently large constant $c$, and large $n$. With probability at least $1- n^2$, a topology $G$ drawn from the $G(n, p)$ distribution is a $(h, p\frac{n}{c})$-expander for all $1 \leq h \leq \frac{1}{p}$ and a $(h, \frac{1}{h}\frac{n}{c})$-expander for all $\frac{1}{p} \leq h \leq \frac{n}{2}$
\end{lemma}

\begin{proof}
	Suppose 
	\begin{equation}\label{eq:GnpExpansionBound}
		|B(S)| \geq \frac{n}{c} \min \left\{ h p, 1 \right\} 
	\end{equation}
	holds for all $S$ such that $1 \leq |S| \leq \frac{n}{2}$. 
	Let $1 \leq h \leq \frac{n}{2}$. We consider 2 cases. 

	\textbf{Case 1.} $1 \leq h \leq \frac{1}{p}$. 

	Consider an arbitrary subset $S$ such that $h' = |S| \leq h$.
	By equation \ref{eq:GnpExpansionBound}, 
	$$
		|B(S)| \geq \frac{n}{c} \min \left\{ h' p, 1 \right\} = \frac{n}{c}h'p = p\frac{n}{c} |S|
	$$
	thus $G$ is a $(h, p\frac{n}{c})$-expander.
	Consider an arbitrary subset $S$ such that $h' = |S| \leq h$. 

	\textbf{Case 2.} $\frac{1}{p} \leq h \leq \frac{n}{2}$

	Consider an arbitrary subset $S$ such that $h' = |S| \leq h$.

	\textbf{Case 2a.}
	Suppose $1 \leq h' \leq \frac{1}{p}$. 
	
	Similarly to Case 1, 
	$$
		|B(S)| \geq p\frac{n}{c} |S|
	$$
	Since $h \geq \frac{1}{p}$, we observe that 
	$$
		|B(S)| \geq \frac{1}{h}  \frac{n}{c} |S|
	$$ 

	\textbf{Case 2b.} Suppose $\frac{1}{p} \leq h' \leq \frac{n}{2}$
	By equation \ref{eq:GnpExpansionBound}, 
	$$
		|B(S)| 
		\geq \frac{n}{c} \min \left\{ h' p, 1 \right\} 
		= \frac{n}{c}\frac{|S|}{h'} 
		\geq \frac{1}{h} \frac{n}{c}|S|
	$$

	Hence, $G$ is a $(h, \frac{1}{h} \frac{n}{c})$-expander.

	By Lemma \ref{lemma:GnpExpansion}, equation \ref{eq:GnpExpansionBound} holds with probability at least $1 - \frac{1}{n^2}$, hence the expansion properties also hold with probability at least $1 - \frac{1}{n^2}$.
\end{proof}

Now we apply Theorem \ref{theorem:markovSyncBound} to the Edge-Markovian Dynamic Network.

\begin{theorem}\label{theorem:edgeMarkovianFloodingBound}
	Let $\mathcal{M}(n, p, q)$ be a stationary Edge-Markovian Dynamic Network. If $\hat{p} = \frac{p}{p+q} \geq c \frac{\log n}{n}$ for a sufficiently large $c$ then w.h.p, the spreading time of Algorithm \ref{SyncFloodingAlgorithm} in $\mathcal{M}(n, p, q)$ is at most
	$$
		\mathcal{O}\left(\frac{\log n}{\log n\hat{p}} + \log \log n\hat{p} \right)
	$$
\end{theorem}

We take a moment to consider the role of $\hat{p}$. Note that since $\hat{p} = \frac{p}{p+q}$ and $\hat{p} \geq c \frac{\log n}{n}$, $p$ and $q$ may be functions of $n$. However, since both quantities are constants once $n$ has been fixed, we omit function notation for brevity.

In the context of the Edge-Markovian model, the larger the ratio between $p$ and $q$, the less often edges fail, and the faster edges recover. %TODO: Could be misinterpreted
This in turn improves the likelihood that a typical topology contains many edges, increasing the speed at which the rumour spreads. This intuition is reflected in the bound given by Theorem \ref{theorem:edgeMarkovianFloodingBound}: the closer $\hat{p}$ is to 1, the smaller the $\frac{\log n}{\log n \hat{p}}$ term will be. However, interestingly the $\log \log n \hat{p}$ term increases with $\hat{p}$, which goes against our intuition, as this term will be larger for denser graphs. This indicates that our bound may not always be tight, which we will explore in Section \ref{subsect:edgeMarkovianFloodingBoundEvaluation}.
%TODO: Restriction means p and q functions of n

\begin{proof}
	Let $\hat{p}$ denote $\frac{p}{p+q}$. 
	By Lemma \ref{lemma:GnpExpansion}, the stationary distribution of $\mathcal{M}(n, p, q)$ is an $(i, k_i)$-expander for $1 \leq i \leq \frac{n}{2}$ where 
	\begin{equation*}
		k_i = 
		\begin{cases*}
			\frac{n\hat{p}}{c} & $1 \leq i \leq \floor{\frac{1}{\hat{p}}}$ \\
			\frac{n}{ci} & $\floor{\frac{1}{\hat{p}}} \leq i \leq \frac{n}{2}$
		\end{cases*}
	\end{equation*}
	By Theorem \ref{theorem:DeterministicFloodingBound} we have that the spreading time is at most
	$$
		\mathcal{O}\left(\sum_{i=1}^\frac{n}{2} \frac{\log \frac{i}{\max\{i-1,1\}}}{\log(1+k_i)}\right)
	$$
	w.h.p.
	Since $\log \frac{i}{i-1} \leq \frac{1}{i}$ for $i \geq 2$, % TODO: Justify
	this quantity is bounded above by 
	$$
		\mathcal{O}\left(\sum_{i=1}^\frac{n}{2} \frac{1}{i\log(1+k_i)}\right)
	$$
	By substituting for $k_i$ in the summation, we have the that the order of the spreading time is bounded w.h.p by
	$$
		\sum_{i=1}^{\floor{1/\hat{p}}} \frac{1}{i\log(1+\frac{n\hat{p}}{c})} 
		+
		\sum_{i=\floor{1/\hat{p}} + 1}^\frac{n}{2} \frac{1}{i\log(1+\frac{n}{ci})} 
	$$
	We bound each of the sums separately.

	\textbf{First Sum.}

	Using that $\sum_{i=1}^m 1/i \leq \log m + 1$, %TODO: Justify
	we observe that
	$$
		\frac{1}{\log(1+\frac{n\hat{p}}{c})} \sum_{i=1}^{\floor{1/\hat{p}}} \frac{1}{i} 
		\leq 
		\frac{\log \frac{1}{\hat{p}} + 1}{\log(1+\frac{n\hat{p}}{c})} 
		= 
		\mathcal{O}\left(\frac{\log n}{\log(n\hat{p})}\right)
	$$
	where the final equality holds since $\hat{p} \geq \frac{c \log n}{\log n}$

	\textbf{Second Sum.}

	We split the second sum again into
	$$	
		\sum_{i=\floor{1/\hat{p}} + 1}^{\ceil{n/c}-1} \frac{1}{i\log(1+\frac{n}{ci})} 
		+ 
		\sum_{i=\ceil{n/c}}^\frac{n}{2} \frac{1}{i\log(1+\frac{n}{ci})} 
	$$
	Note that
	\begin{align*}
		\sum_{i=\floor{1/\hat{p}} + 1}^{\ceil{n/c}-1} \frac{1}{i\log(1+\frac{n}{ci})} 
		& \leq \sum_{i=\floor{1/\hat{p}} + 1}^{\ceil{n/c}-1} \frac{1}{i\log(\frac{n}{ci})} \\
		& \leq \int_{\floor{1/\hat{p}}}^{\ceil{n/c}-1} \frac{1}{x\log(1+\frac{n}{cx})} dx \\
		\intertext{since $f(x) = x\log(1+\frac{n}{cx})$ is non-negative and concave on the interval $[\floor{1/\hat{p}}, \ceil{n/c}]$ }
		& = \left[-\log\log \frac{n}{cx}\right]_{\floor{1/\hat{p}}}^{\ceil{n/c}-1} \\
		&= \mathcal{O}(\log \log (n \hat{p}))
	\end{align*}
	Since $\log(1+x) \geq \frac{x}{1+x}$ for $x < 1$ we observe that
	\begin{align*}
		\sum_{i=\ceil{n/c}}^\frac{n}{2} \frac{1}{i\log(1+\frac{n}{ci})} 
		& \leq 
		\sum_{i=\ceil{n/c}}^\frac{n}{2} \frac{(1 + \frac{n}{ci})}{i\frac{n}{ci}} 
		= 
		\sum_{i=\ceil{n/c}}^\frac{n}{2} \left( \frac{c}{n} + \frac{1}{i} \right) \\
		& \leq 
		\sum_{i=\ceil{n/c}}^\frac{n}{2} \left( \frac{c}{n} + \frac{c}{n} \right)
		= \mathcal{O}(1)
	\end{align*}

	To obtain the result, we recombine the summations
	$$
		\mathcal{O}\left(\sum_{i=1}^\frac{n}{2} \frac{\log \frac{i}{\max\{i-1,1\}}}{\log(1+k_i)}\right) =
		\mathcal{O}\left(\frac{\log n}{\log (n\hat{p})} + \log \log (n\hat{p})\right)
	$$

\end{proof}

Theorem \ref{theorem:edgeMarkovianFloodingBound} holds if $\hat{p}$ is at least $c \frac{\log n}{n}$ for a sufficiently large constant $c$, but what happens if $\hat{p} = o(\frac{\log n}{n})$? % TODO: Is mixing little o and concrete inequalities confusing?
It turns out that in this case we are unable to apply Theorem \ref{theorem:markovSyncBound}, so the assumption that $\hat{p} \geq c \frac{\log n}{n}$ is necessary for the proof as well as sufficient.

For the $G(n, \hat{p})$ distribution, it is well known that if $\hat{p} = o(\frac{\log n}{n})$, then w.h.p the graph will be disconnected \cite{complexNetworksRandomGraphs}. Since the graph can be split into at least two connected components, we notice that the smaller component is a set of size at most $\frac{n}{2}$ with an empty boundary. Hence, w.h.p, the graph is not a $(\frac{n}{2}, k)$-expander for any positive $k$. This is a condition for Theorem \ref{theorem:markovSyncBound}, hence we cannot apply the bound in cases where $\hat{p} = o(\frac{\log n}{n})$.

Bounding the spreading time of the Edge-Markovian Network in cases where $\hat{p} = o(\frac{\log n}{n})$ could be an interesting area for further study. Each topology will be disconnected w.h.p, but unlike the static $G(n,p)$ graph, the changing edges allow the rumour to spread between sets of nodes which were previously disconnected, and eventually inform all nodes in the network.
% TODO: Define "w.h.p"

\subsection{Evaluating the quality of the bound}\label{subsect:edgeMarkovianFloodingBoundEvaluation}

% TODO: Note \hat{p} is a function of n
% lower bound on mixing time
% TODO: Why can't generalise to varying t? (Non-homogenous MC)

In this subsection we evaluate how tight the bound in Theorem \ref{theorem:edgeMarkovianFloodingBound} is, using lower bounds on the spreading time and simulations. % TODO: Rephrase 
We start with a lower bound on the spreading time of Algorithm \ref{SyncFloodingAlgorithm} on the stationary Edge-Markovian Network.

\begin{theorem}
	Let $\mathcal{M}(n, p, q)$ be a stationary Edge-Markovian Dynamic Network. If $\hat{p} = \frac{p}{p+q} \geq c \frac{\log n}{n}$ for a sufficiently large $c$ then w.h.p, the spreading time of Algorithm \ref{SyncFloodingAlgorithm} in $\mathcal{M}(n, p, q)$ is at least 
	$$
		\Omega\left(\frac{\log n}{\log (n \hat{p})}\right)
	$$
\end{theorem}

\begin{proof}
	Let $\delta_{v,t}$ be the degree of node $v \in V$ at time $t$. Since each topology is distributed according to the ER $G(n, \hat{p})$ distribution, $\delta_{v,t} \sim \text{Binomial}(n-1,\hat{p})$, as each of the $n-1$ other nodes have probability $\hat{p}$ of sharing an edge with $v$ in $G_t$. Thus,
	\begin{align*}
		\mathbb{P}(\delta_{v,t} \geq 2n\hat{p}) 
		& \leq \mathbb{P}(\text{Binomial}(n, \hat{p}) \geq 2n\hat{p}) \\ 
		\intertext{since the $\text{Binomial}(n, \hat{p})$ distribution stochastically dominates the $\text{Binomial}(n-1, \hat{p})$ distribution} % TODO: Prove in prelims and cite
		& \leq \exp(-\frac{n\hat{p}}{3}) \\
		\intertext{by the Chernoff bound. We omit the proof of the Chernoff bound for brevity, but refer the reader to \cite{binomialChernoff} for a full proof.} % TODO: Move earlier if used there
		& \leq \exp(-\frac{c \log n}{3}) = n^{-\frac{c}{3}}
		\intertext{since $\hat{p} \geq \frac{c \log n}{n}$}
	\end{align*}
	By taking $c=9$, we obtain $\mathbb{P}(\delta_{v,t} > 2n\hat{p}) \leq n^{-3}$.
	We use this result to bound the probability that for a given time $t$, there exists a node with degree at least $2n\hat{p}$.
	$$
		\mathbb{P}\left(\bigcup_{v \in V} \left\{ \delta_{v,t} \geq 2n\hat{p}\right\} \right) 
		\leq 
		\sum_{v \in V}
		\mathbb{P}(\delta_{v,t} \geq 2n\hat{p}) 
		=
		n \mathbb{P}(\delta_{v,t} \geq 2n\hat{p}) 
		\leq
		n^{-2} 
	$$
	Denote maximum degree of $G_t$ as $\Delta_t$. Thus, by the previous union bound, the probability that $\Delta_t$ is at least $2n\hat{p}$ is at most $n^{-2}$ for all $t$. Now we use another union bound to bound the probability that the maximum degree of the network is at least $2n\hat{p}$ at any time step in the first $n$ rounds. % TODO: Exceeds is wrong word.
	$$
		\mathbb{P}\left(\bigcup_{t \in [n]} \left\{ \Delta_t > 2n\hat{p}\right\} \right) 
		\leq 
		\sum_{t \in [n]}
		\mathbb{P}(\Delta_t \geq 2n\hat{p}) 
		=
		n \mathbb{P}(\Delta_t \geq 2n\hat{p}) 
		\leq
		\frac{1}{n}
	$$
	Hence, w.h.p, in the first $n$ time steps all nodes have degree less than $2n\hat{p}$.

	For the remainder of the proof, we assume the high probability scenario where all nodes have degree less than $2n\hat{p}$ for the first $n$ rounds.
	Observe that in the first $n$ rounds, each node can spread the rumour to at most $2n\hat{p} - 1$ other nodes, since it has at most $2n\hat{p} - 1$ neighbours.
	Thus, in round $t$ the number of newly informed nodes is at most $(2n\hat{p} - 1)|I_t|$, since each of the $|I_t|$ informed nodes can spread the rumour to at most $2n\hat{p} - 1$ uninformed nodes. Hence, by induction at time $t < n$ the number of informed nodes is at most $(2n\hat{p})^t$.

	\textbf{Base Case.} $t=0$.

	Initially only one node is aware of the rumour, so $|I_0| = 1 = (2n\hat{p})^0$, hence the claim holds in this case.

	\textbf{Inductive Case.} $t=k+1$.

	We assume that $|I_k| \leq (2n\hat{p})^k$. Then 
	$$
		|I_{k+1}| \leq |I_k| + (2n\hat{p} - 1)|I_k| = (2n\hat{p})|I_k| \leq (2n\hat{p})^{k+1}
	$$
	by the induction hypothesis, hence the claim holds in this case.

	Let $\tau = \frac{\log (n/2)}{\log(2n\hat{p})}$. Note that for sufficiently large $n$, $\tau < n$. Hence, we obtain that for all times $t < \tau = \frac{\log (n/2)}{\log(2n\hat{p})}$, the number of informed nodes is at most $\frac{n}{2}$. Thus, the spreading time is at least 
	$$
		\tau = \Omega\left(\frac{\log n}{\log(n \hat{p})}\right)
	$$
\end{proof}

% TODO: DEFINE WHP in prelims

% TODO: implications, bound tight when log n/ log(np) is dominant term,
 
% TODO: simulations outside tight range p like ((logn)^k)/n

\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{./figures/flooding_gnp_simulation_results.png}
	\caption{Results of Edge-Markovian Dynamic Network spreading time simulations}
	\label{fig:floodingGnpSimResults}
\end{figure}

% TODO: Tight in means, takes discrete values